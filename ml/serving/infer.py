# ml/serving/infer.py
from __future__ import annotations

import json
import os
import sys
from typing import Any, Dict, List, Optional, Tuple

import joblib
import numpy as np
import pandas as pd

# Ensure project root for imports
_PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if _PROJECT_ROOT not in sys.path:
    sys.path.insert(0, _PROJECT_ROOT)

# Optional (live MT5 fallback), safe to import if present
try:
    import gpt_agent as ga  # engine for MT5 and helpers
except Exception:
    ga = None  # type: ignore

# Optional feature builder fallback
try:
    from ml.features.build_features import make_features
except Exception:
    make_features = None  # type: ignore

def _latest_version_dir(registry_dir: str, symbol: str, timeframe: str, h: int) -> Optional[str]:
    base = os.path.join(registry_dir, symbol, timeframe.upper(), str(int(h)))
    if not os.path.isdir(base):
        return None
    subs = [
        d for d in os.listdir(base)
        if os.path.isdir(os.path.join(base, d)) and d.startswith("v")
    ]
    if not subs:
        return None
    subs.sort(key=lambda v: os.path.getmtime(os.path.join(base, v)), reverse=True)
    return os.path.join(base, subs[0])

# ---------------- paths & loaders ----------------
def _active_dir(registry_dir: str, symbol: str, timeframe: str, h: int) -> Optional[str]:
    """
    Resolve active model directory for (symbol, TF, horizon).
    Prefer {h}/active.json → {h}/{version}/
    Fallback: pick most recent {h}/vYYYYMMDD_HHMMSS by mtime.
    """
    base = os.path.join(registry_dir, symbol, timeframe.upper(), str(int(h)))
    ap = os.path.join(base, "active.json")
    if os.path.isfile(ap):
        try:
            with open(ap, "r", encoding="utf-8") as f:
                meta = json.load(f) or {}
            ver = meta.get("version")
            if ver:
                d = os.path.join(base, ver)
                if os.path.isdir(d):
                    return d
        except Exception:
            pass  # fall through to latest-by-mtime
    # fallback: latest version folder by mtime
    return _latest_version_dir(registry_dir, symbol, timeframe, h)


def _load_model_and_schema(version_dir: str) -> Tuple[Any, List[str]]:
    model_path = os.path.join(version_dir, "model_calibrated.joblib")
    schema_path = os.path.join(version_dir, "feature_schema.json")
    if not os.path.isfile(model_path):
        raise FileNotFoundError(f"model_calibrated.joblib not found at {model_path}")
    if not os.path.isfile(schema_path):
        raise FileNotFoundError(f"feature_schema.json not found at {schema_path}")
    model = joblib.load(model_path)
    with open(schema_path, "r", encoding="utf-8") as f:
        schema = json.load(f) or {}
    features = schema.get("features") or []
    if not features:
        raise ValueError("feature_schema.json has empty or missing 'features' list")
    return model, [str(c) for c in features]


def _features_cache_path(data_dir: str, symbol: str, timeframe: str) -> str:
    return os.path.join(data_dir, "features", symbol, f"{timeframe.upper()}.parquet")


def _latest_features_from_cache(
    data_dir: str, symbol: str, timeframe: str
) -> Optional[pd.Series]:
    """
    Preferred path: use the cached features generated by feature_agent.
    Ensures we are using the exact same transformations used during training.
    """
    p = _features_cache_path(data_dir, symbol, timeframe)
    if not os.path.isfile(p):
        return None
    try:
        df = pd.read_parquet(p)
        if df is None or df.empty:
            return None
        # last completed feature row
        return df.iloc[-1]
    except Exception:
        return None


def _latest_features_from_live(symbol: str, timeframe: str, lookback_days: int = 180) -> Optional[pd.Series]:
    """
    Fallback: pull bars from MT5 via gpt_agent and build features ad-hoc.
    Only used if cache isn’t available.
    """
    if ga is None or make_features is None:
        return None
    try:
        # Project engine helper (your code usually has a variant like this)
        # If your engine uses a different name, wire it here.
        copy_fn = getattr(ga, "_copy_rates_recent_df", None)
        if copy_fn is None:
            return None
        df = copy_fn(symbol, timeframe, int(lookback_days))
        if df is None or df.empty:
            return None
        cols = [c for c in ["time_utc", "time", "open", "high", "low", "close", "volume"] if c in df.columns]
        base = df[cols].copy()
        # Try to build features with your pipeline
        feats = make_features(base, timeframe)  # type: ignore[arg-type]
        if feats is None or feats.empty:
            return None
        return feats.iloc[-1]
    except Exception:
        return None


def _row_from_series(series: pd.Series, feature_cols: List[str]) -> pd.DataFrame:
    """
    Create a single-row DataFrame matching the model's expected columns.
    Missing columns become 0.0.
    """
    vals = []
    for c in feature_cols:
        v = pd.to_numeric(series.get(c), errors="coerce")
        if pd.isna(v) or v is None or v is False:
            v = 0.0
        vals.append(float(v))
    row = pd.DataFrame([vals], columns=feature_cols)
    row = row.replace([np.inf, -np.inf], np.nan).fillna(0.0)
    return row


def _active_eval_metric(registry_dir: str, symbol: str, timeframe: str, h: int, metric: str = "balanced_accuracy") -> float:
    """Read eval.json for the active model of a horizon and return the metric value. Returns -inf if unavailable."""
    ver_dir = _active_dir(registry_dir, symbol, timeframe, h)
    if not ver_dir:
        return float("-inf")
    path = os.path.join(ver_dir, "eval.json")
    try:
        with open(path, "r", encoding="utf-8") as f:
            obj = json.load(f)
        m = obj.get("metrics", {}) or {}
        return float(m.get(metric, m.get("balanced_accuracy", float("-inf"))))
    except Exception:
        return float("-inf")


def _proba(model, X: pd.DataFrame) -> Dict[str, float]:
    # Map probabilities to labels regardless of class_ ordering
    try:
        proba = model.predict_proba(X)[0]
        classes = list(getattr(model, "classes_", []))
        out = {str(c): float(proba[i]) for i, c in enumerate(classes)}
        # Normalize keys we expect
        canonical = {"buy": 0.0, "sell": 0.0, "none": 0.0}
        for k, v in out.items():
            kk = str(k).strip().lower()
            if kk in canonical:
                canonical[kk] = float(v)
        # If model used numeric classes
        if sum(canonical.values()) == 0.0:
            # heuristics for {1: buy, -1: sell, 0: none}
            canonical["buy"] = float(out.get("1", out.get(1, 0.0)))
            canonical["sell"] = float(out.get("-1", out.get(-1, 0.0)))
            canonical["none"] = float(out.get("0", out.get(0, 0.0)))
        # Renormalize (safety)
        s = sum(canonical.values()) or 1.0
        for k in canonical:
            canonical[k] = canonical[k] / s
        return canonical
    except Exception:
        return {"buy": 0.0, "sell": 0.0, "none": 1.0}


# ---------------- public API ----------------
def infer_once(
    symbol: str,
    timeframe: str,
    h: int,
    registry_dir: str = os.path.join("ml", "models", "registry"),
    data_dir: str = os.path.join("ml", "data"),
    lookback_days: int = 180,
) -> Optional[Dict[str, Any]]:
    """
    Run a single (symbol, timeframe, horizon) inference and return a strategy-like row:
      {strategy, decision, confidence, timeframe, as_of_utc, extras}
    Uses cached features first; falls back to live MT5 build.
    """
    ver_dir = _active_dir(registry_dir, symbol, timeframe, h)
    if not ver_dir:
        return None

    model, feature_cols = _load_model_and_schema(ver_dir)

    # 1) cached features (preferred)
    last_series = _latest_features_from_cache(data_dir, symbol, timeframe)

    # 2) live MT5 fallback
    if last_series is None:
        last_series = _latest_features_from_live(symbol, timeframe, lookback_days=lookback_days)
        if last_series is None:
            return None

    # Build X row in model order
    X = _row_from_series(last_series, feature_cols)
    probs = _proba(model, X)
    buy_p = probs.get("buy", 0.0)
    sell_p = probs.get("sell", 0.0)
    decision = "buy" if buy_p >= sell_p else "sell"
    confidence = float(max(buy_p, sell_p))

    # time column best effort
    as_of = None
    for tcol in ("time_utc", "time"):
        if tcol in last_series.index:
            as_of = str(last_series.get(tcol))
            break

    return {
        "strategy": "Arvid v1",
        "decision": decision,
        "confidence": confidence,
        "timeframe": timeframe,
        "as_of_utc": as_of,
        "extras": {
            "model": "Arvid v1",
            "horizon": int(h),
            "probs": probs,
            "version_dir": ver_dir,
            "source": "cache" if _features_cache_path(data_dir, symbol, timeframe) else "live",
        },
    }


def infer_timeframes(
    symbol: str,
    timeframes: List[str],
    horizons_map: Dict[str, List[int]],
    registry_dir: str = os.path.join("ml", "models", "registry"),
    data_dir: str = os.path.join("ml", "data"),
    lookback_days: int = 180,
    select_policy: str = "best_eval",
    metric: str = "balanced_accuracy",
) -> List[Dict[str, Any]]:
    """
    Returns one or more Arvid v1 rows per timeframe depending on select_policy.
    Typical UI use: select_policy='best_eval' → one row per TF.
    """
    out: List[Dict[str, Any]] = []
    for tf in timeframes:
        hs_all = [int(h) for h in horizons_map.get(tf.upper(), [])]
        if not hs_all:
            continue

        policy = (select_policy or "best_eval").lower()
        if policy == "all":
            hs = hs_all
        elif policy == "longest":
            hs = [max(hs_all)]
        elif policy == "shortest":
            hs = [min(hs_all)]
        elif policy == "best_eval":
            best_h = None
            best_val = float("-inf")
            for h in hs_all:
                val = _active_eval_metric(registry_dir, symbol, tf, h, metric=metric)
                if val > best_val:
                    best_val, best_h = val, h
            hs = [best_h if best_h is not None else max(hs_all)]
        else:
            hs = hs_all

        for h in hs:
            r = infer_once(
                symbol,
                tf,
                h,
                registry_dir=registry_dir,
                data_dir=data_dir,
                lookback_days=lookback_days,
            )
            if r:
                out.append(r)
    return out
